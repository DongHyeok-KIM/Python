{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tenserflow\n",
    "# Multi_Valriable_linear_regression_study\n",
    "# 다변량 선형 회귀\n",
    "\n",
    "### 필요한 모듈 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈 세팅\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#텐서플로우 버전 확인\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.random.set.seed()\n",
    "초기값을 지정해주는 역활\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    a = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "    b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "    print(a.numpy(),b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "    b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "    print(a.numpy(),b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2변량 선형회귀 -1.변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set.seed()는 변수, 여기선 W1,W2값을 고정해 주는 역활.\n",
    "# 해당 코드를 몇번을 반복해도 같은 값을 돌려줌.\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "#변수 x1, x3 선언\n",
    "x1_data = [5.,0.,3.,4.,5.]\n",
    "x2_data = [4.,3.,1.,2.,0.]\n",
    "#정답인 y값 선언\n",
    "y_data  = [1.,2.,3.,4.,5.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2변량 선형회귀 -2.W초기값, learning_rate할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#변수가 2개이기 때문에 W(가중치, 기울기)도 2개를 선언\n",
    "#tf.random.uniform 균등분포를 따르는 난수를 생성, -10~10사이의수\n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -10, 10.0))\n",
    "\n",
    "learning_rate=tf.Variable(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2변량 선형회귀 -3.경사하강법으로 cost, W1,W2,b도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순 선형회귀에서 사용한 경사하강법과 동일\n",
    "for i in range(1000+1):\n",
    "    #tape에 모든 with연산기록 저장\n",
    "    with tf.GradientTape() as tape:\n",
    "        #hypothesis(추론,공식)을 전해줍니다.\n",
    "        hypothesis = W1 * x1_data + W2* x2_data+b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1,W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 ==0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2변량 선형회귀(메트릭스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이번엔 같은 예제를 메트릭스를 활용하여 도출.\n",
    "#메트릭스를 사용하지 않았을때(위예시)와 비교하면서 확인.\n",
    "#x와, y데이터를 따로따로 주었습니다.\n",
    "x_data = [\n",
    "    [5.,0.,3.,4.,5.],\n",
    "    [4.,3.,1.,2.,0.]\n",
    "]\n",
    "\n",
    "y_data = [1.,2.,3.,4.,5.]\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1,2), -10.0, 10.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b \n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 50 ==0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3변량 단순 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#값 고정 역활\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# 데이터 세팅\n",
    "x1 = [ 60.,  91.,  78.,  64.,  74.]\n",
    "x2 = [ 70.,  87.,  60.,  80.,  67.]\n",
    "x3 = [ 73.,  92.,  93., 99.,  71.]\n",
    "Y  = [130., 170., 160., 170., 150.]\n",
    "\n",
    "# 변수만큼 w갯수 세팅\n",
    "w1 = tf.Variable(tf.random.normal((1,)))\n",
    "w2 = tf.Variable(tf.random.normal((1,)))\n",
    "w3 = tf.Variable(tf.random.normal((1,)))\n",
    "b  = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "\n",
    "learning_rate = 0.000001\n",
    "print(\"epoch | cost\")\n",
    "#경사하강법\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1* x1 + w2*x2 + w3*x3 + b\n",
    "        cost =tf.reduce_mean(tf.square(hypothesis -Y))\n",
    "        \n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1,w2,w3,b])\n",
    "    \n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3변량 단순 선형회귀(메트릭스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np배열로 x와 y 선언\n",
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 60.,  70.,  73., 130. ],\n",
    "    [ 91.,  87.,  92., 170. ],\n",
    "    [ 78.,  60.,  93., 160. ],\n",
    "    [ 64.,  80., 99., 170. ],\n",
    "    [ 74.,  67.,  71., 150. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# 슬라이스로, 엑스와, y를 명확히 선언및 대입\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "#x의 변수가 3개인것을 주의\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 1000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 ==0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[1][0], b.numpy()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변량 선형회귀 요약\n",
    "다변량 선형회귀는 변수를 직접 선언하는것과, 메트릭스를 통해 선언 하는 두가지 방법이 있다.\n",
    "\n",
    "전자는 변수가 많을 경우 하나하나 모두 세팅을 해줘야하는 반면,   \n",
    "후자는 변수가 많아도 메트릭스를 사용해 한번만 세팅이 가능하다.\n",
    "\n",
    "더 빠른 결과, 정확도가 높은 결과를 도출 할 수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변량 선형회귀 확인 및 예측(바로 위 예제를 통한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y값 확인\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X값 확인\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b값 확인\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict공식을 통한 해 도출\n",
    "predict(X).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1,x2,x3값 할당한 후 예측값\n",
    "predict([[ 1.,  1.,  4.]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2개의 해 동시 도출\n",
    "predict([[ 1.,  1.,  4.],[ 145.,  50.,  50.]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3개의 해 동시 도출\n",
    "#특히 마지막 배열은 최초 선언시 주었던 x값임\n",
    "predict([[ 1.,  1.,  4.],[ 145.,  50.,  50.],[ 74.,  67.,  71.]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
