{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinormial Logistic regression\n",
    "## softmax Classifier\n",
    "\n",
    "\n",
    "\n",
    "### 필요한 모듈 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 세팅했습니다.\n",
    "#x데이터는 4개씩 8슬라이스로 나뉜 데이터를 준비했습니다.\n",
    "x_data = [[1, 2, 4, 1],\n",
    "          [2, 2, 3, 2],\n",
    "          [3, 7, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 1, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [2, 5, 6, 7],\n",
    "          [1, 7, 7, 7]]\n",
    "#y데이터는 3개씩 8개 슬라이스로 나뉜 데이터를 준비했구요,\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [1, 0, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 0, 1],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "#즉 [1,2,4,1] =[0,0,1] 처럼 대치되는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4, 1],\n",
       " [2, 2, 3, 2],\n",
       " [3, 7, 3, 4],\n",
       " [4, 1, 5, 5],\n",
       " [1, 7, 1, 5],\n",
       " [1, 2, 5, 6],\n",
       " [2, 5, 6, 7],\n",
       " [1, 7, 7, 7]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 4., 1.],\n",
       "       [2., 2., 3., 2.],\n",
       "       [3., 7., 3., 4.],\n",
       "       [4., 1., 5., 5.],\n",
       "       [1., 7., 1., 5.],\n",
       "       [1., 2., 5., 6.],\n",
       "       [2., 5., 6., 7.],\n",
       "       [1., 7., 7., 7.]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서플로우 함수를 이용하기전에 데이터를 넘피 배열로 변환 해줍니다\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서플로우 함수를 이용하기전에 데이터를 넘피 배열로 변환 해줍니다\n",
    "y_data = np.array(y_data, dtype =np.float32)\n",
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 4), (None, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습시클 값을 dataset에 담아준다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "#repeat()은 반복, batch()는 묶음을 의미한다.\n",
    "#반복을 통해 학습할 데이터의 양을 늘려준다.\n",
    "#묶음을 통해 학습속도?를 조절 한다.\n",
    "dataset = dataset.repeat(2).batch(2)\n",
    "#dataset = dataset.batch(2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[1., 2., 4., 1.],\n",
      "       [2., 2., 3., 2.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[3., 7., 3., 4.],\n",
      "       [4., 1., 5., 5.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [0., 1., 0.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[1., 7., 1., 5.],\n",
      "       [1., 2., 5., 6.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[0., 1., 0.],\n",
      "       [0., 0., 1.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[2., 5., 6., 7.],\n",
      "       [1., 7., 7., 7.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [1., 0., 0.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[1., 2., 4., 1.],\n",
      "       [2., 2., 3., 2.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[3., 7., 3., 4.],\n",
      "       [4., 1., 5., 5.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [0., 1., 0.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[1., 7., 1., 5.],\n",
      "       [1., 2., 5., 6.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[0., 1., 0.],\n",
      "       [0., 0., 1.]], dtype=float32)>), (<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[2., 5., 6., 7.],\n",
      "       [1., 7., 7., 7.]], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [1., 0., 0.]], dtype=float32)>)]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#dataset값 확인하기\n",
    "#repeat과 batch에 따라 len(elem)이 달라지는것을 확인 가능하다.\n",
    "elem = [i for i in dataset]\n",
    "print(elem)\n",
    "print(len(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 갯수 선언\n",
    "현재 y값을 [0,0,1]과같이 3개분류로 나누었기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분류가짓수를 3으로 선언\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#즉 y값의 열과 같은값입니다.\n",
    "print(y_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W,b 할당해 주기\n",
    "softmax를 사용을 위해, W값을 배열((x값의 열값),(y값의 열값)으로 마춰줘야한다.   \n",
    "b값은 ((y값의 열값),)으로 맞춰준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)),name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
       "array([[ 0.09651633, -0.69214475, -0.49522713],\n",
       "       [ 0.15072225,  2.0019443 , -0.70925987],\n",
       "       [-0.7252303 ,  0.23252739, -2.033938  ],\n",
       "       [ 0.21821043,  0.5606956 ,  1.0457467 ]], dtype=float32)>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.5649487 , -0.44032323,  0.35004106], dtype=float32)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[ 0.09651633, -0.69214475, -0.49522713],\n",
       "        [ 0.15072225,  2.0019443 , -0.70925987],\n",
       "        [-0.7252303 ,  0.23252739, -2.033938  ],\n",
       "        [ 0.21821043,  0.5606956 ,  1.0457467 ]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.5649487 , -0.44032323,  0.35004106], dtype=float32)>]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그리고 변수값에 담아준다.\n",
    "variables = [W,b]\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax함수\n",
    "변수를 sotfmax함수에 적용시킨후 값을 리턴 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.9039767e-04 9.9970931e-01 2.2574139e-07]\n",
      " [2.1250849e-03 9.9778324e-01 9.1741429e-05]\n",
      " [2.5288910e-02 9.7468692e-01 2.4201512e-05]\n",
      " [2.1250849e-03 9.9778324e-01 9.1741429e-05]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "testarr = [[5,6,1,3],[1,2,3,4],[4,3,2,1],[1,2,3,4]]\n",
    "testarr = np.array(testarr, dtype = np.float32)\n",
    "\n",
    "print(hypothesis(testarr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost함수\n",
    "cost(에러율,오차)을 찾는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.610207, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X,Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis =1 )\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.00101466, -0.00313479,  0.0021202 ],\n",
      "       [-0.00215435,  0.00115918,  0.00099536],\n",
      "       [-0.00142222,  0.00508382, -0.00366149],\n",
      "       [ 0.00123679, -0.0037799 ,  0.00254326]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.00488178, -0.00058013, -0.00430162], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "\n",
    "        return grads\n",
    "\n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복 및 결과 값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.026241\n",
      "Loss at epoch 1000: 0.017634\n",
      "Loss at epoch 2000: 0.013287\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=1000):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
       "array([[-1.2719332 ,  2.5360806 , -2.3549967 ],\n",
       "       [ 2.3983405 , -0.47378036, -0.4811593 ],\n",
       "       [ 0.41838354, -5.3999457 ,  2.4549253 ],\n",
       "       [-0.56878024,  4.0097694 , -1.6163393 ]], dtype=float32)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00419643 0.00761949 0.9881841 ]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "print(tf.argmax(a, 1)) #index: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.3917855e-03 1.8417474e-09 9.9860829e-01]\n",
      " [4.5423996e-02 6.9806455e-03 9.4759536e-01]\n",
      " [9.7841245e-01 2.1580271e-02 7.2500425e-06]\n",
      " [3.7434872e-04 9.7464120e-01 2.4984468e-02]\n",
      " [2.5994429e-02 9.7400552e-01 2.1482348e-08]\n",
      " [1.8975602e-02 1.7405065e-02 9.6361935e-01]\n",
      " [9.6871400e-01 3.2455898e-03 2.8040400e-02]\n",
      " [9.9671632e-01 8.1378770e-09 3.2836695e-03]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 0 1 1 2 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 0 1 1 2 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1)) # matches with y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter = ',', dtype=np.float32)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1]\n",
    "\n",
    "nb_classes = 7\n",
    "\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 3.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 6.],\n",
       "       [0., 1., 1., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xy.shape)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 3., 0., 0., 0., 0., 3., 3., 0., 0., 1., 3., 6., 6., 6., 1.,\n",
       "       0., 3., 0., 1., 1., 0., 1., 5., 4., 4., 0., 0., 0., 5., 0., 0., 1.,\n",
       "       3., 0., 0., 1., 3., 5., 5., 1., 5., 1., 0., 0., 6., 0., 0., 0., 0.,\n",
       "       5., 4., 6., 0., 0., 1., 1., 1., 1., 3., 3., 2., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 6., 3., 0., 0., 2., 6., 1., 1., 2., 6., 3., 1., 0.,\n",
       "       6., 3., 1., 5., 4., 2., 2., 3., 0., 0., 1., 0., 5., 0., 6., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(101, 7), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, \n",
    "                                                      from_logits=True)    \n",
    "    cost = tf.reduce_mean(cost_i)    \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 3.799741268157959, Acc: 0.2772277295589447\n",
      "Steps: 100 Loss: 0.5048637986183167, Acc: 0.8415841460227966\n",
      "Steps: 200 Loss: 0.30533459782600403, Acc: 0.9306930899620056\n",
      "Steps: 300 Loss: 0.22628118097782135, Acc: 0.9603960514068604\n",
      "Steps: 400 Loss: 0.18192091584205627, Acc: 0.9603960514068604\n",
      "Steps: 500 Loss: 0.1528809368610382, Acc: 0.9801980257034302\n",
      "Steps: 600 Loss: 0.1321880966424942, Acc: 0.9900990128517151\n",
      "Steps: 700 Loss: 0.11662240326404572, Acc: 0.9900990128517151\n",
      "Steps: 800 Loss: 0.10445830971002579, Acc: 0.9900990128517151\n",
      "Steps: 900 Loss: 0.09467662870883942, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.08663266897201538, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
