{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic_Regression(Classification)\n",
    "# 로지스틱 회귀(분류)\n",
    "\n",
    "로지스틱 회귀란  = 독립 변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는 통계기법(출처 나무위키)    \n",
    "\n",
    "흔히 로지스틱 회귀는 종속변수가 2개(이항, boolean, True or False)인경우를 말하며, 이를 Classification 분류 기법이라고 한다.\n",
    "\n",
    "종속변수가 2개이상일 경우 다항로지스틱 회귀라고 한다.(Multinormial logisitic regression).\n",
    "\n",
    "\n",
    "### 필요한 모듈 세팅 및 tensorflow버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as pn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1, x2를 각각 x_train의 2차원배열로 정의해줍니다.\n",
    "x_train = [[1., 6.],\n",
    "          [2., 5.],\n",
    "          [3., 4.],\n",
    "          [4., 3.],\n",
    "          [5., 2.],\n",
    "          [6., 1.]]\n",
    "\n",
    "#각 배열의 도출 값을 선언합니다.\n",
    "#1,6 -> 0\n",
    "#6,1 -> 1\n",
    "\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 6.0], [2.0, 5.0], [3.0, 4.0], [4.0, 3.0], [5.0, 2.0], [6.0, 1.0]] [[0.0], [0.0], [0.0], [1.0], [1.0], [1.0]]\n"
     ]
    }
   ],
   "source": [
    "#데이터 확인\n",
    "print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 할 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test할 데이터를 세팅해 줍니다.\n",
    "#2,6이 1인지 확인하기 위합니다.\n",
    "x_test = [[2.,6.]]\n",
    "y_test = [[0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2.0, 6.0]], [[0.0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x값 할당해 주기\n",
    "x_train에 있던 값들을 각각 x1, x2로 할당\n",
    "이때, 리스트 형식으로 할당,\n",
    "복습차원에서 리스트 컴프리헨션과, append방법 두가지를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [x[0] for x in x_train]\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=[]\n",
    "for x in range(len(x_train)):\n",
    "    x1.append(x_train[x][0])\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0, 5.0, 4.0, 3.0, 2.0, 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = [x[1] for x in x_train]\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0, 5.0, 4.0, 3.0, 2.0, 1.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2=[]\n",
    "for x in range(len(x_train)):\n",
    "    x2.append(x_train[x][1])\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 그래프로 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = []\n",
    "for y in range(len(y_train)):\n",
    "    colors.append(int(y_train[y][0]))\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [int(y[0]) for y in y_train]\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#0은 보라색, 1은 노란색으로 표시가 됩니다.\n",
    "colors = [int(y[0]) for y in y_train]\n",
    "\n",
    "#x1, x2에 해당되는 값들은 삼각형으로 표시\n",
    "plt.scatter(x1,x2, c= colors, marker='^')\n",
    "\n",
    "#제가 찾고자 했던 x_test값 (2,6)은 빨간색으로 표시했습니다.\n",
    "plt.scatter(x_test[0][0], x_test[0][1], c='red')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((2,), (1,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset 만들기\n",
    "#학습시킬 값을 dataset에 담아준다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 살펴보기\n",
    "현재 dataset은 6슬라이스로 이루어져 있다.    \n",
    "1슬라이스 => [1,6],[0]   \n",
    "2슬라이스 => [2,5],[0] ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 6.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 4.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([4., 3.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([5., 2.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 1.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#리스트 컴프리헨션\n",
    "elem = [i for i in dataset]\n",
    "print(elem)\n",
    "print(len(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 6.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 4.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([4., 3.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([5., 2.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 1.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append로 확인하기\n",
    "elem2= []\n",
    "for i in dataset:\n",
    "    elem2.append(i)\n",
    "print(elem2)\n",
    "len(elem2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W, b 할당해 주기\n",
    "W,b초기값을 할당해 줍니다.\n",
    "x가 x1,x2 2개의 변수였기 때문에 W는 2행1열의 배열을 선언해준다.    \n",
    "b는 1행 1열로 선언해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.1278012 ]\n",
      " [ 0.42648628]] [-1.4933363]\n"
     ]
    }
   ],
   "source": [
    "#초기값을 tf.random.normal로 설정해주기\n",
    "W = tf.Variable(tf.random.normal((2, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "print(W.numpy(),b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]] [0.]\n"
     ]
    }
   ],
   "source": [
    "#초기값을 tf.zeros로 설정해 주기\n",
    "W = tf.Variable(tf.zeros([2,1]), name= 'weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "print(W.numpy(),b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost 함수 구현, 러닝레이트 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1-labels) * tf.math.log(1-hypothesis))\n",
    "    return cost\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate =0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과값 도출 함수 구현\n",
    "시그모이드 함수로 도출된 hypothesis를 0 or 1로 cast해준다.   \n",
    "\n",
    "\n",
    "tf.cast => 조건이 참일경우 1, 거짓일 경우 0출력   \n",
    "tf.equal => 주어인 값이 같은경우 True, 다를경우 False   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 0.3\n",
    "test2 = tf.cast(test1 > 0.5,dtype = tf.float32)\n",
    "test2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 0.6\n",
    "test2 = tf.cast(test1 > 0.5,dtype = tf.float32)\n",
    "test2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 0.6\n",
    "test2 = 0.6\n",
    "test3 = tf.equal(test1,test2)\n",
    "test3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4 = 0.5\n",
    "test5 = 0.6\n",
    "test6 = tf.equal(test4,test5)\n",
    "test6.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test7 = tf.cast(tf.equal(test1,test2),dtype = tf.float32)\n",
    "test7.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test8 = tf.cast(tf.equal(test4,test5),dtype = tf.float32)\n",
    "test8.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype =tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels),dtype =tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법(미분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "        return tape.gradient(loss_value, [W,b])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복 및 결과 값 도출\n",
    "어큐러시가 1이 나왔음으로,   \n",
    "위에서 제시했던 test값은 참임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0, Loss: 0.0908\n",
      "Iter : 100, Loss: 0.0863\n",
      "Iter : 200, Loss: 0.0823\n",
      "Iter : 300, Loss: 0.0787\n",
      "Iter : 400, Loss: 0.0755\n",
      "Iter : 500, Loss: 0.0726\n",
      "Iter : 600, Loss: 0.0699\n",
      "Iter : 700, Loss: 0.0675\n",
      "Iter : 800, Loss: 0.0652\n",
      "Iter : 900, Loss: 0.0631\n",
      "Iter : 1000, Loss: 0.0612\n",
      "Testset Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "#반복 횟수 선언\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    #dataset을 몇 묶음씩 학습 시킬 것인지 정해준다\n",
    "    #batch size를 선언해줄 수 있다.\n",
    "    for features, labels in iter(dataset.batch(len(x_train))):\n",
    "        #print(features)\n",
    "        #print(labels)\n",
    "        hypothesis = logistic_regression(features)\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars =zip(grads,[W,b]))\n",
    "        if step % 100 ==0:\n",
    "            print(\"Iter : {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test값 변환 후 결과 값 도출\n",
    "어큐러시가 0이 나왔음으로 새롭게 제시한 test값은 거짓임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = [[2.,6.]]\n",
    "y_test2 = [[1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0, Loss: 0.0470\n",
      "Iter : 100, Loss: 0.0460\n",
      "Iter : 200, Loss: 0.0450\n",
      "Iter : 300, Loss: 0.0440\n",
      "Iter : 400, Loss: 0.0431\n",
      "Iter : 500, Loss: 0.0422\n",
      "Iter : 600, Loss: 0.0414\n",
      "Iter : 700, Loss: 0.0406\n",
      "Iter : 800, Loss: 0.0398\n",
      "Iter : 900, Loss: 0.0391\n",
      "Iter : 1000, Loss: 0.0384\n",
      "Testset Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#반복 횟수 선언\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    #dataset을 몇 묶음씩 학습 시킬 것인지 정해준다\n",
    "    #batch size를 선언해줄 수 있다.\n",
    "    for features, labels in iter(dataset.batch(len(x_train))):\n",
    "        #print(features)\n",
    "        #print(labels)\n",
    "        hypothesis = logistic_regression(features)\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars =zip(grads,[W,b]))\n",
    "        if step % 100 ==0:\n",
    "            print(\"Iter : {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test2),y_test2)\n",
    "print(\"Testset Accuracy: {:4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch값 설정\n",
    "batch값을 2로 설정 하였는데,   \n",
    "아래 예 처럼 6개의 슬라이스를 3번에 나눠 각각 학습이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0, Loss: 0.0009\n",
      "Iter : 0, Loss: 0.1133\n",
      "Iter : 0, Loss: 0.0009\n",
      "Iter : 100, Loss: 0.0007\n",
      "Iter : 100, Loss: 0.1076\n",
      "Iter : 100, Loss: 0.0007\n",
      "Iter : 200, Loss: 0.0006\n",
      "Iter : 200, Loss: 0.1024\n",
      "Iter : 200, Loss: 0.0006\n",
      "Iter : 300, Loss: 0.0005\n",
      "Iter : 300, Loss: 0.0977\n",
      "Iter : 300, Loss: 0.0005\n",
      "Iter : 400, Loss: 0.0005\n",
      "Iter : 400, Loss: 0.0934\n",
      "Iter : 400, Loss: 0.0005\n",
      "Iter : 500, Loss: 0.0004\n",
      "Iter : 500, Loss: 0.0895\n",
      "Iter : 500, Loss: 0.0004\n",
      "Iter : 600, Loss: 0.0004\n",
      "Iter : 600, Loss: 0.0858\n",
      "Iter : 600, Loss: 0.0004\n",
      "Iter : 700, Loss: 0.0003\n",
      "Iter : 700, Loss: 0.0825\n",
      "Iter : 700, Loss: 0.0003\n",
      "Iter : 800, Loss: 0.0003\n",
      "Iter : 800, Loss: 0.0794\n",
      "Iter : 800, Loss: 0.0003\n",
      "Iter : 900, Loss: 0.0003\n",
      "Iter : 900, Loss: 0.0765\n",
      "Iter : 900, Loss: 0.0003\n",
      "Iter : 1000, Loss: 0.0002\n",
      "Iter : 1000, Loss: 0.0739\n",
      "Iter : 1000, Loss: 0.0002\n",
      "Testset Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#반복 횟수 선언\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    #dataset을 몇 묶음씩 학습 시킬 것인지 정해준다\n",
    "    #batch size를 선언해줄 수 있다.\n",
    "    for features, labels in iter(dataset.batch(2)):\n",
    "        #print(features)\n",
    "        #print(labels)\n",
    "        hypothesis = logistic_regression(features)\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars =zip(grads,[W,b]))\n",
    "        if step % 100 ==0:\n",
    "            print(\"Iter : {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test2),y_test2)\n",
    "print(\"Testset Accuracy: {:4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
